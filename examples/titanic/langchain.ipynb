{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "langchain_chat_kwargs = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 4000,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "chat_openai_model_kwargs = {\n",
    "    \"top_p\": 1.0,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "    \"presence_penalty\": -1,\n",
    "}\n",
    "\n",
    "\n",
    "# Optional: set the API key for OpenAI if it's not set in the environment.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "def get_chat_openai(model_name):\n",
    "    llm = ChatOpenAI(\n",
    "        model_name=model_name,\n",
    "        model_kwargs=chat_openai_model_kwargs,\n",
    "        **langchain_chat_kwargs\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up imports of local module code\n",
    "import os\n",
    "nb_path = os.path.abspath(\"\")\n",
    "from sys import path\n",
    "from os.path import dirname\n",
    "\n",
    "path.insert(0,  dirname(dirname(nb_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__preql_internal.concept_name\n",
      "__preql_internal.datasource\n",
      "__preql_internal.query_text\n",
      "passenger.id\n",
      "passenger.id.count\n",
      "passenger.age\n",
      "passenger.survived\n",
      "passenger.name\n",
      "passenger.class\n",
      "passenger.fare\n",
      "passenger.cabin\n",
      "passenger.embarked\n",
      "passenger.ticket\n",
      "passenger.last_name\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from preql import Executor, Dialects\n",
    "from preql.core.models import Environment\n",
    "from sqlalchemy import create_engine\n",
    "from preql.core.models import Datasource, Concept, ColumnAssignment, Grain, Function\n",
    "from preql.core.enums import DataType, Purpose, FunctionType\n",
    "from os.path import dirname\n",
    "from pathlib import PurePath\n",
    "from preql.parsing.render import render_environment\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def setup_engine() -> Executor:\n",
    "    engine = create_engine(r\"duckdb:///:memory:\", future=True)\n",
    "    csv = PurePath(nb_path) / \"train.csv\"\n",
    "    df = pd.read_csv(csv)\n",
    "    _ = df\n",
    "    output = Executor(engine=engine, dialect=Dialects.DUCK_DB)\n",
    "\n",
    "    output.execute_raw_sql(\"CREATE TABLE raw_titanic AS SELECT * FROM df\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def setup_titanic(env: Environment):\n",
    "    namespace = \"passenger\"\n",
    "    id = Concept(\n",
    "        name=\"id\", namespace=namespace, datatype=DataType.INTEGER, purpose=Purpose.KEY\n",
    "    )\n",
    "    age = Concept(\n",
    "        name=\"age\",\n",
    "        namespace=namespace,\n",
    "        datatype=DataType.INTEGER,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        keys=[id],\n",
    "    )\n",
    "\n",
    "    name = Concept(\n",
    "        name=\"name\",\n",
    "        namespace=namespace,\n",
    "        datatype=DataType.STRING,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        keys=[id],\n",
    "    )\n",
    "\n",
    "    pclass = Concept(\n",
    "        name=\"class\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.INTEGER,\n",
    "        keys=[id],\n",
    "    )\n",
    "    survived = Concept(\n",
    "        name=\"survived\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.BOOL,\n",
    "        keys=[id],\n",
    "    )\n",
    "    fare = Concept(\n",
    "        name=\"fare\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.FLOAT,\n",
    "        keys=[id],\n",
    "    )\n",
    "    embarked = Concept(\n",
    "        name=\"embarked\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.INTEGER,\n",
    "        keys=[id],\n",
    "    )\n",
    "    cabin = Concept(\n",
    "        name=\"cabin\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.STRING,\n",
    "        keys=[id],\n",
    "    )\n",
    "    ticket = Concept(\n",
    "        name=\"ticket\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.STRING,\n",
    "        keys=[id],\n",
    "    )\n",
    "\n",
    "    last_name = Concept(\n",
    "        name=\"last_name\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.STRING,\n",
    "        keys=[id],\n",
    "        lineage=Function(\n",
    "            operator=FunctionType.INDEX_ACCESS,\n",
    "            arguments=[\n",
    "                Function(\n",
    "                    operator=FunctionType.SPLIT,\n",
    "                    arguments=[name, \",\"],\n",
    "                    output_datatype=DataType.ARRAY,\n",
    "                    output_purpose=Purpose.PROPERTY,\n",
    "                    arg_count=2,\n",
    "                ),\n",
    "                1,\n",
    "            ],\n",
    "            output_datatype=DataType.STRING,\n",
    "            output_purpose=Purpose.PROPERTY,\n",
    "            arg_count=2,\n",
    "        ),\n",
    "    )\n",
    "    all_concepts = [\n",
    "        id,\n",
    "        age,\n",
    "        survived,\n",
    "        name,\n",
    "        pclass,\n",
    "        fare,\n",
    "        cabin,\n",
    "        embarked,\n",
    "        ticket,\n",
    "        last_name,\n",
    "    ]\n",
    "    for x in all_concepts:\n",
    "        env.add_concept(x)\n",
    "\n",
    "    env.add_datasource(\n",
    "        Datasource(\n",
    "            identifier=\"raw_data\",\n",
    "            address=\"raw_titanic\",\n",
    "            columns=[\n",
    "                ColumnAssignment(alias=\"passengerid\", concept=id),\n",
    "                ColumnAssignment(alias=\"age\", concept=age),\n",
    "                ColumnAssignment(alias=\"survived\", concept=survived),\n",
    "                ColumnAssignment(alias=\"pclass\", concept=pclass),\n",
    "                ColumnAssignment(alias=\"name\", concept=name),\n",
    "                ColumnAssignment(alias=\"fare\", concept=fare),\n",
    "                ColumnAssignment(alias=\"cabin\", concept=cabin),\n",
    "                ColumnAssignment(alias=\"embarked\", concept=embarked),\n",
    "                ColumnAssignment(alias=\"ticket\", concept=ticket),\n",
    "            ],\n",
    "            grain=Grain(components=[id]),\n",
    "        ),\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "def create_passenger_dimension(exec: Executor, name: str):\n",
    "    exec.execute_raw_sql(f\"CREATE SEQUENCE seq_{name} START 1;\")\n",
    "    exec.execute_raw_sql(\n",
    "        f\"\"\"create table dim_{name} as \n",
    "                         SELECT passengerid id, name, age,\n",
    "                          SPLIT(name, ',')[1] last_name\n",
    "                            FROM raw_data\n",
    "\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_arbitrary_dimension(exec: Executor, key: str, name: str):\n",
    "    exec.execute_raw_sql(\n",
    "        f\"\"\"create table dim_{name} as \n",
    "                         with tmp as \n",
    "                         (select {key}\n",
    "                         from raw_data group by 1\n",
    "                         )\n",
    "                         SELECT  row_number() over() as id,\n",
    "                         {key} as {name}\n",
    "                          FROM tmp\n",
    "\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_fact(\n",
    "    exec: Executor,\n",
    "    dims: Optional[list[str]] = None,\n",
    "    include: Optional[list[str]] = None,\n",
    "):\n",
    "    exec.execute_raw_sql(\n",
    "        \"\"\"create table fact_titanic as \n",
    "                         SELECT \n",
    "                         row_number() OVER () as fact_key,\n",
    "                         passengerid,\n",
    "                         survived,\n",
    "                         fare,\n",
    "                         embarked,\n",
    "                         b.id class_id,\n",
    "                         cabin  \n",
    "                         FROM raw_data a \n",
    "                         LEFT OUTER JOIN dim_class b on a.pclass=b.class\n",
    "                         \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def setup_normalized_engine() -> Executor:\n",
    "    engine = create_engine(r\"duckdb:///:memory:\", future=True)\n",
    "    csv = PurePath(dirname(nb_path)) / \"train.csv\"\n",
    "    df = pd.read_csv(csv)\n",
    "    _ = df\n",
    "    output = Executor(engine=engine, dialect=Dialects.DUCK_DB)\n",
    "\n",
    "    output.execute_raw_sql(\"CREATE TABLE raw_data AS SELECT * FROM df\")\n",
    "    create_passenger_dimension(output, \"passenger\")\n",
    "    create_arbitrary_dimension(output, \"pclass\", \"class\")\n",
    "    create_fact(output, [\"passenger\"])\n",
    "    return output\n",
    "\n",
    "\n",
    "def setup_titanic_distributed(env: Environment):\n",
    "    namespace = \"passenger\"\n",
    "    id = Concept(\n",
    "        name=\"id\", namespace=namespace, datatype=DataType.INTEGER, purpose=Purpose.KEY\n",
    "    )\n",
    "    age = Concept(\n",
    "        name=\"age\",\n",
    "        namespace=namespace,\n",
    "        datatype=DataType.INTEGER,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        keys=[id],\n",
    "    )\n",
    "\n",
    "    name = Concept(\n",
    "        name=\"name\",\n",
    "        namespace=namespace,\n",
    "        datatype=DataType.STRING,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        keys=[id],\n",
    "    )\n",
    "    class_id = Concept(\n",
    "        name=\"_class_id\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.KEY,\n",
    "        datatype=DataType.INTEGER,\n",
    "        # keys=[id],\n",
    "    )\n",
    "    pclass = Concept(\n",
    "        name=\"class\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.INTEGER,\n",
    "        keys=[class_id],\n",
    "    )\n",
    "    survived = Concept(\n",
    "        name=\"survived\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.BOOL,\n",
    "        keys=[id],\n",
    "    )\n",
    "    survived = Concept(\n",
    "        name=\"survived\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.BOOL,\n",
    "        keys=[id],\n",
    "    )\n",
    "    fare = Concept(\n",
    "        name=\"fare\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.FLOAT,\n",
    "        keys=[id],\n",
    "    )\n",
    "    embarked = Concept(\n",
    "        name=\"embarked\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.INTEGER,\n",
    "        keys=[id],\n",
    "    )\n",
    "    cabin = Concept(\n",
    "        name=\"cabin\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.STRING,\n",
    "        keys=[id],\n",
    "    )\n",
    "    last_name = Concept(\n",
    "        name=\"last_name\",\n",
    "        namespace=namespace,\n",
    "        purpose=Purpose.PROPERTY,\n",
    "        datatype=DataType.STRING,\n",
    "        keys=[id],\n",
    "        lineage=Function(\n",
    "            operator=FunctionType.INDEX_ACCESS,\n",
    "            arguments=[\n",
    "                Function(\n",
    "                    operator=FunctionType.SPLIT,\n",
    "                    arguments=[name, \",\"],\n",
    "                    output_datatype=DataType.ARRAY,\n",
    "                    output_purpose=Purpose.PROPERTY,\n",
    "                    arg_count=2,\n",
    "                ),\n",
    "                1,\n",
    "            ],\n",
    "            output_datatype=DataType.STRING,\n",
    "            output_purpose=Purpose.PROPERTY,\n",
    "            arg_count=2,\n",
    "        ),\n",
    "    )\n",
    "    for x in [id, age, survived, name, pclass, fare, cabin, embarked, last_name]:\n",
    "        env.add_concept(x)\n",
    "\n",
    "    env.add_datasource(\n",
    "        Datasource(\n",
    "            identifier=\"dim_passenger\",\n",
    "            address=\"dim_passenger\",\n",
    "            columns=[\n",
    "                ColumnAssignment(alias=\"id\", concept=id),\n",
    "                ColumnAssignment(alias=\"age\", concept=age),\n",
    "                ColumnAssignment(alias=\"name\", concept=name),\n",
    "                ColumnAssignment(alias=\"last_name\", concept=last_name),\n",
    "                # ColumnAssignment(alias=\"pclass\", concept=pclass),\n",
    "                # ColumnAssignment(alias=\"name\", concept=name),\n",
    "                # ColumnAssignment(alias=\"fare\", concept=fare),\n",
    "                # ColumnAssignment(alias=\"cabin\", concept=cabin),\n",
    "                # ColumnAssignment(alias=\"embarked\", concept=embarked),\n",
    "            ],\n",
    "            grain=Grain(components=[id]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    env.add_datasource(\n",
    "        Datasource(\n",
    "            identifier=\"fact_titanic\",\n",
    "            address=\"fact_titanic\",\n",
    "            columns=[\n",
    "                ColumnAssignment(alias=\"passengerid\", concept=id),\n",
    "                ColumnAssignment(alias=\"survived\", concept=survived),\n",
    "                ColumnAssignment(alias=\"class_id\", concept=class_id),\n",
    "                ColumnAssignment(alias=\"fare\", concept=fare),\n",
    "                ColumnAssignment(alias=\"cabin\", concept=cabin),\n",
    "                ColumnAssignment(alias=\"embarked\", concept=embarked),\n",
    "            ],\n",
    "            grain=Grain(components=[id]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    env.add_datasource(\n",
    "        Datasource(\n",
    "            identifier=\"dim_class\",\n",
    "            address=\"dim_class\",\n",
    "            columns=[\n",
    "                ColumnAssignment(alias=\"id\", concept=class_id),\n",
    "                ColumnAssignment(alias=\"class\", concept=pclass),\n",
    "                # ColumnAssignment(alias=\"fare\", concept=fare),\n",
    "                # ColumnAssignment(alias=\"cabin\", concept=cabin),\n",
    "                # ColumnAssignment(alias=\"embarked\", concept=embarked),\n",
    "            ],\n",
    "            grain=Grain(components=[class_id]),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return env\n",
    "\n",
    "\n",
    "from trilogy_public_models import models\n",
    "from preql import Dialects\n",
    "from preql_nlp.main_v2 import build_query\n",
    "from preql.hooks.query_debugger import DebuggingHook\n",
    "from logging import StreamHandler, DEBUG\n",
    "\n",
    "from preql_nlp.constants import logger\n",
    "\n",
    "executor = setup_engine()\n",
    "\n",
    "env = Environment()\n",
    "model = setup_titanic(env)\n",
    "for c in env.concepts:\n",
    "    print(c)\n",
    "\n",
    "executor.environment = env\n",
    "environment = env\n",
    "logger.setLevel(DEBUG)\n",
    "logger.addHandler(StreamHandler())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool, Tool\n",
    "from datetime import datetime\n",
    "import ast\n",
    "import json\n",
    "from pydantic import BaseModel\n",
    "from preql.core.enums import ComparisonOperator, Ordering\n",
    "from preql.core.models import Concept\n",
    "\n",
    "class FilterResult(BaseModel):\n",
    "    \"\"\"The result of the filter prompt\"\"\"\n",
    "\n",
    "    column: str\n",
    "    values: list[str|int |float|bool]\n",
    "    operator: ComparisonOperator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class OrderResult(BaseModel):\n",
    "    \"\"\"The result of the order prompt\"\"\"\n",
    "\n",
    "    column: str\n",
    "    order: Ordering\n",
    "\n",
    "\n",
    "class InitialParseResponse(BaseModel):\n",
    "    \"\"\"The result of the initial parse\"\"\"\n",
    "\n",
    "    columns: list[str]\n",
    "    limit: Optional[int] = 100\n",
    "    order: Optional[list[OrderResult]] = None\n",
    "    filtering: Optional[list[FilterResult]] = None\n",
    "\n",
    "    @property\n",
    "    def selection(self) -> list[str]:\n",
    "        filtering = [f.concept for f in self.filtering]\n",
    "        order = [x.concept for x in self.order]\n",
    "        return list(set(self.metrics + self.dimensions + filtering + order))\n",
    "\n",
    "\n",
    "\n",
    "def run_query_save_results(query:str):\n",
    "    executor.execute_query(query)\n",
    "\n",
    "def validate_query(query:str):\n",
    "    return json.dumps({'validated':True})\n",
    "    parsed = InitialParseResponse.model_validate_json(query)\n",
    "    for x in parsed.fields:\n",
    "        if x not in environment.concepts:\n",
    "            raise ValueError(f\"{x} in output columns not in concepts\")\n",
    "    for y in parsed.order:\n",
    "        if y.concept not in environment.concepts:\n",
    "            raise ValueError(f\"{y.concept} in ordering not in concepts\")\n",
    "    for z in parsed.filtering:\n",
    "        if z.concept not in environment.concepts:\n",
    "            raise ValueError(f\"{z.concept} in filtering not in concepts\")\n",
    "    \n",
    "def get_dataset_description(query:str):\n",
    "    \"\"\"\n",
    "    Get the description of the dataset.\n",
    "    \"\"\"\n",
    "    return json.dumps({'description':'This is a dataset contain the details of a subset of the passengers on board the Titanic (891 to be exact) and importantly, will reveal whether they survived or not, and some other information about them.'})\n",
    "\n",
    "\n",
    "def get_today_date(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Useful to get the date of today.\n",
    "    \"\"\"\n",
    "    # Getting today's date in string format\n",
    "    today_date_string = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    return today_date_string\n",
    "\n",
    "def concept_to_string(concept:Concept):\n",
    "    return concept.address\n",
    "\n",
    "def get_concepts(*args, **kwargs) -> str:\n",
    "    return json.dumps({'concepts':[concept_to_string(x) for \n",
    "                                   x in environment.concepts.values()]})\n",
    "\n",
    "def sql_agent_tools():\n",
    "    tools = [\n",
    "                Tool.from_function(\n",
    "            func=get_dataset_description,\n",
    "            name=\"get_dataset_description\",\n",
    "            description=\"\"\"\n",
    "            Get some context on the datasets you're working with, if the question is not clear\"\"\",\n",
    "        ),\n",
    "        Tool.from_function(\n",
    "            func=validate_query,\n",
    "            name=\"validate_response\",\n",
    "            description=\"\"\"\n",
    "            Check that your response is formatted properly and accurate before sending it to the CEO.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Tool.from_function(\n",
    "            func=get_concepts,\n",
    "            name=\"get_concepts\",\n",
    "            description=\"\"\"\n",
    "            The list of fields that can be selected.\n",
    "            \"\"\",\n",
    "        ),\n",
    "        Tool.from_function(\n",
    "            func=get_today_date,\n",
    "            name=\"get_today_date\",\n",
    "            description=\"\"\"\n",
    "            Useful to get the date of today.\n",
    "            \"\"\",\n",
    "        ),\n",
    "    ]\n",
    "    return tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_structured_chat_agent, AgentType\n",
    "from langchain.prompts import PromptTemplate, Prompt\n",
    "\n",
    "llm_agent = get_chat_openai('gpt-3.5-turbo-1106')\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system = '''Thought Process: You are an analyst. Your job is to get questions from \n",
    "the CEO and tell them how to write a \n",
    "SQL query to answer them in a step by step fashion. \n",
    "You have access to a single predefined sql table and have to pick the best columns from \n",
    "it to answer the question; you cannot create or derive any new concepts. \n",
    "If nothing seems like it will accurately answer the question, \n",
    "you should tell the CEO that his question can't be answered yet. \n",
    "\n",
    "Your goal will be to create a summary of steps in JSON format for the CEO. \n",
    "The final output should be a VALID JSON blob with the following keys and values followed by a stopword: <EOD>:\n",
    "- columns: a list of columns\n",
    "- limit: a number of records to limit the results to, -1 if none specified\n",
    "- order: a list of  columns to order the results by, with the option to specify ascending or descending\n",
    "-- column: a column name to order by\n",
    "-- order: the direction of ordering, \"asc\" or \"desc\"\n",
    "- filtering: a list of all objects to filter the results on, where each object has the following keys:\n",
    "-- column: a column to filter on\n",
    "-- values: the value the column is filtered to\n",
    "-- operator: the comparison operator, one of \"=\", \"in\", \"<\", \">\", \"<=\", \"like\", or \">=\". A range, or between, should be expressed as two inequalities. \n",
    "\n",
    "\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "\n",
    "You should always call the the validate_response tool with your final answer before sending it to the CEO.\n",
    "\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\n",
    "\n",
    "```\n",
    "{{\n",
    "    \"action\": $TOOL_NAME,\n",
    "    \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "\n",
    "Follow this format:\n",
    "\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation'''\n",
    "\n",
    "human = '''{input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\n",
    "(reminder to respond in a JSON blob no matter what)'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "agent = create_structured_chat_agent(\n",
    "    llm=llm_agent,\n",
    "    tools=sql_agent_tools(),\n",
    "    # suffix=CUSTOM_SUFFIX,\n",
    "    prompt =  prompt,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor, create_structured_chat_agent\n\u001b[0;32m      3\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(\n\u001b[0;32m      4\u001b[0m     agent\u001b[38;5;241m=\u001b[39mllm_agent, tools\u001b[38;5;241m=\u001b[39msql_agent_tools(), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, handle_parsing_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mList the first names of survivors on the titanic?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\coding_projects\\preql-nlp\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:133\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m include_run_info \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_run_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    131\u001b[0m return_only_outputs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_only_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 133\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[0;32m    135\u001b[0m     callbacks,\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[0;32m    142\u001b[0m )\n\u001b[0;32m    143\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ethan\\coding_projects\\preql-nlp\\.venv\\Lib\\site-packages\\langchain\\chains\\base.py:479\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;66;03m# If there are multiple input keys, but some get set by memory so that\u001b[39;00m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# only one is not set, we can still figure out which key it is.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m         _input_keys \u001b[38;5;241m=\u001b[39m _input_keys\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mmemory_variables)\n\u001b[1;32m--> 479\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_input_keys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m: inputs}\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m     external_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mload_memory_variables(inputs)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=llm_agent, tools=sql_agent_tools(), verbose=True, handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "result = agent_executor.invoke( \"List the first names of survivors on the titanic?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'How many passengers survived in each class?', 'output': \"I need to select the 'passenger_class' and 'survived' columns from the dataset. Then I can group the data by 'passenger_class' and count the number of passengers who survived in each class.\"}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I need to select the 'passenger_class' and 'survived' columns from the \"\n",
      " \"dataset. Then I can group the data by 'passenger_class' and count the number \"\n",
      " 'of passengers who survived in each class.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(result['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for InitialParseResponse\n  Input should be a valid dictionary or instance of InitialParseResponse [type=model_type, input_value=\"I'm unable to retrieve t...nd their relationships.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/model_type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mInitialParseResponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ethan\\coding_projects\\preql-nlp\\.venv\\Lib\\site-packages\\pydantic\\main.py:509\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[1;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    508\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for InitialParseResponse\n  Input should be a valid dictionary or instance of InitialParseResponse [type=model_type, input_value=\"I'm unable to retrieve t...nd their relationships.\", input_type=str]\n    For further information visit https://errors.pydantic.dev/2.6/v/model_type"
     ]
    }
   ],
   "source": [
    "InitialParseResponse.model_validate(result['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
